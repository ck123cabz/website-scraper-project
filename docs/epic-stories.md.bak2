# Website Scraper Platform - Epic Breakdown

**Author:** CK
**Date:** 2025-10-13
**Project Level:** Level 2 - Small Complete System
**Target Scale:** 1-2 epics, 5-15 stories, 8-12 week timeline

---

## Epic Overview

This Level 2 project focuses on modernizing an existing Python scraper into a production-ready NestJS platform with **PRIMARY emphasis on real-time UI/UX transparency**. The two epics are designed for overlapping development: Epic 1 (Dashboard) begins first to establish UI foundation, Epic 2 (Pipeline) builds backend in parallel.

**Total Stories:** 15 stories across 3 epics
**Estimated Timeline:** 12-14 weeks
**Key Constraint:** Solo developer with AI assistance, no authentication required

---

## Epic Details

## Epic 1: Real-Time Transparency Dashboard

**Epic Goal:** Create comprehensive real-time monitoring dashboard where multiple team members can simultaneously view scraping operations with live logs, progress indicators, current URL display, and historical results - providing complete transparency into system operations.

**Priority:** P0 (Must Have)
**Timeline:** Weeks 1-6
**Story Count:** 7 stories
**Story Points:** ~21 points

**Why This Epic Matters:**
This is the PRIMARY differentiator from the old Python system. Team needs visibility into operations - not just final results. Real-time transparency enables team collaboration, faster debugging, and trust in the system.

**Technical Foundation:**
- React + TypeScript + shadcn/ui
- Supabase Realtime subscriptions for live updates
- WebSocket connections for log streaming
- Optimistic UI updates for responsive feel

---

### Story 1.1: Job Dashboard Foundation

**As a** team member
**I want to** see a clean dashboard showing all current and past scraping jobs
**So that** I can quickly understand system state and access any job

**Acceptance Criteria:**
- [ ] Dashboard displays list of all jobs (active, paused, completed)
- [ ] Each job card shows: name, status badge, start time, progress percentage, URL count
- [ ] Active jobs appear at top with visual distinction (color, animation)
- [ ] Clicking job card navigates to detailed job view
- [ ] "New Job" button prominently displayed (primary CTA)
- [ ] Empty state shown when no jobs exist with helpful onboarding message
- [ ] Dashboard updates automatically when job status changes (via Supabase Realtime)

**Story Points:** 3
**Dependencies:** None (foundation story)

---

### Story 1.2: Live Progress Tracking

**As a** team member
**I want to** see real-time progress indicators for active jobs
**So that** I can monitor processing without refreshing or wondering if system is working

**Acceptance Criteria:**
- [ ] Progress bar shows percentage complete (0-100%) updating in real-time
- [ ] Counter displays: "Processed: X / Y URLs"
- [ ] Processing rate displayed: "XX URLs/min" (calculated from recent throughput)
- [ ] Time indicators: "Elapsed: HH:MM:SS" and "Est. Remaining: HH:MM:SS"
- [ ] Success/failure counters: "Success: X | Failed: Y"
- [ ] All metrics update every 1-2 seconds via Supabase subscription
- [ ] Visual "pulse" indicator shows system is actively processing
- [ ] Progress bar color changes based on success rate (green >95%, yellow >80%, red <80%)

**Story Points:** 3
**Dependencies:** Story 1.1

---

### Story 1.3: Current URL Display Panel

**As a** team member
**I want to** see exactly which URL is currently being processed with its status
**So that** I can track progress at granular level and identify any stuck URLs

**Acceptance Criteria:**
- [ ] Dedicated panel shows: "Currently Processing: [URL]"
- [ ] Processing stage displayed: "Stage: Fetching | Filtering | Classifying"
- [ ] Stage indicator uses visual icons (spinner, filter icon, AI icon)
- [ ] Time on current URL displayed: "Processing for: XX seconds"
- [ ] Previous 3 URLs shown below current with final status (âœ“ or âœ—)
- [ ] URL truncated if too long with tooltip showing full URL
- [ ] Panel updates immediately when URL changes (<500ms latency)
- [ ] Empty state: "Waiting to start..." when job paused or not started

**Story Points:** 2
**Dependencies:** Story 1.2

---

### Story 1.4: Live Activity Log Streaming

**As a** team member
**I want to** see a live scrolling log of all system activities
**So that** I can understand exactly what's happening and debug issues in real-time

**Acceptance Criteria:**
- [ ] Scrollable log panel displays activity feed with auto-scroll to latest
- [ ] Each log entry shows: timestamp, severity icon, message
- [ ] Severity levels: SUCCESS (âœ“ green), INFO (â„¹ blue), WARNING (âš  yellow), ERROR (âœ— red)
- [ ] Log entries include:
  - URL fetch started/completed
  - Pre-filter decisions with reasoning ("PASS - Sending to LLM", "REJECT - Blog platform")
  - LLM API calls ("Gemini classification: SUITABLE (score: 0.87)")
  - Errors with details ("ScrapingBee 429 - Rate limit, retrying in 30s")
  - Cost updates ("$0.045 - GPT fallback used")
- [ ] Logs stream in real-time with <1 second latency
- [ ] Auto-scroll can be paused by user scroll, resume with "Jump to latest" button
- [ ] Log entries persist during session, cleared when job completed and viewed
- [ ] Filter controls: "Show: All | Errors Only | Info Only"

**Story Points:** 5
**Dependencies:** Story 1.3

---

### Story 1.5: Cost Tracking Display

**As a** team member
**I want to** see real-time cost tracking for LLM API usage
**So that** I can monitor budget and understand cost per job

**Acceptance Criteria:**
- [ ] Cost panel displays: "Total Cost: $XX.XX"
- [ ] Cost per URL displayed: "$X.XXXXX/URL"
- [ ] Provider breakdown: "Gemini: $XX.XX | GPT: $XX.XX"
- [ ] Projected total cost: "Projected: $XX.XX" (based on remaining URLs Ã— avg cost/URL)
- [ ] Savings indicator: "40% saved vs GPT-only" (if pre-filtering working)
- [ ] Cost updates in real-time as URLs processed
- [ ] Historical job costs shown in job list
- [ ] Warning shown if projected cost exceeds $50 (configurable threshold)

**Story Points:** 3
**Dependencies:** Story 1.2

---

### Story 1.6: Historical Results Table

**As a** team member
**I want to** view searchable table of all processed URLs with results
**So that** I can review classifications and reference past results

**Acceptance Criteria:**
- [ ] Data table shows columns: URL, Status, Classification, Score, Cost, Processing Time, Timestamp
- [ ] Table supports sorting by any column (ascending/descending)
- [ ] Search/filter bar: search by URL text
- [ ] Filter dropdowns: Status (All | Success | Failed), Classification (All | SUITABLE | NOT_SUITABLE)
- [ ] Pagination: 50 results per page with page controls
- [ ] Table updates in real-time as new URLs processed
- [ ] Row click expands to show: full URL, classification reasoning, API calls made, error details (if failed)
- [ ] "Export" button to download filtered results
- [ ] Table persists across page refreshes (data from Supabase)

**Story Points:** 4
**Dependencies:** Story 1.4

---

### Story 1.7: Job Control Actions

**As a** team member
**I want to** pause, resume, or cancel active jobs
**So that** I can control processing and respond to issues

**Acceptance Criteria:**
- [ ] Control buttons displayed for active jobs: "Pause", "Cancel"
- [ ] Paused jobs show: "Resume", "Cancel"
- [ ] Pause button immediately stops processing new URLs (current URL completes)
- [ ] UI updates to "Paused" state instantly with optimistic update
- [ ] Resume button continues from last processed URL
- [ ] Cancel button shows confirmation: "Cancel job? Processed results will be saved."
- [ ] Cancelled jobs marked as "Cancelled" with results preserved
- [ ] All control actions broadcast via Supabase - all connected users see state change
- [ ] Disabled states: can't pause/resume when system is already transitioning
- [ ] Tooltips explain what each action does

**Story Points:** 3
**Dependencies:** Story 1.1, 1.2

---

## Epic 2: Production-Grade Processing Pipeline

**Epic Goal:** Implement robust NestJS + BullMQ queue architecture with 3-tier progressive filtering (domain analysis â†’ operational validation â†’ LLM classification), cost-optimized classification with confidence-based routing, and reliable job processing with automatic retries and persistence.

**Priority:** P0 (Must Have)
**Timeline:** âœ… COMPLETE (Weeks 3-8) - Requires refactoring for 3-tier architecture (Weeks 14-16)
**Story Count:** 6 stories
**Story Points:** ~25 points

**Why This Epic Matters:**
Replaces basic Python threading with production-grade queue system. Enables scalability, reliability, cost optimization through 3-tier progressive filtering (60-70% LLM cost savings + 40-60% scraping cost savings), and proper error handling. Foundation for all processing features.

**Note:** Epic 2 implementation completed (Week 8) based on V1 single-pass pipeline logic. Sprint Change Proposal approved 2025-10-16 requires refactoring Stories 2.3-2.5 and adding NEW Story 2.6 for 3-tier progressive filtering architecture. Refactoring scheduled for Weeks 14-16.

**Technical Foundation:**
- NestJS + TypeScript backend
- BullMQ + Redis for job queue
- Supabase PostgreSQL for persistence
- ScrapingBee API for scraping
- Gemini 2.0 Flash + GPT-4o-mini APIs
- 3-tier progressive filtering architecture

---

### Story 2.1: NestJS Backend Foundation & Job Queue Setup

**As a** developer
**I want to** set up NestJS backend with BullMQ queue integration
**So that** we have production-grade architecture for job processing

**Acceptance Criteria:**
- [ ] NestJS application initialized with TypeScript
- [ ] BullMQ configured with Redis connection (Railway managed Redis)
- [ ] Job queue created: "url-processing-queue"
- [ ] Bull Board dashboard configured for dev monitoring (at /admin/queues)
- [ ] Supabase client configured with environment variables
- [ ] Database tables created:
  - `jobs` (id, status, created_at, updated_at, url_count, processed_count, etc.)
  - `urls` (id, job_id, url, status, classification, cost, processing_time, etc.)
  - `logs` (id, job_id, timestamp, severity, message)
- [ ] Health check endpoint: GET /health
- [ ] Basic job endpoints: POST /jobs (create), GET /jobs/:id (status)
- [ ] Deployed to Railway with auto-deployment on git push
- [ ] Environment variables configured in Railway

**Story Points:** 5
**Dependencies:** None (foundation story for Epic 2)

---

### Story 2.2: Bulk URL Upload & Job Creation

**As a** team member
**I want to** upload URLs via file or textarea and create scraping job
**So that** I can start processing my URL list

**Acceptance Criteria:**
- [ ] POST /jobs/create endpoint accepts:
  - File upload (CSV, TXT) via multipart/form-data
  - JSON body with `urls` array
  - Text body with line-separated URLs
- [ ] CSV parser handles: single column, multi-column (auto-detect URL column), headers/no headers
- [ ] URL validation: basic format check, remove empty lines, trim whitespace
- [ ] Deduplication: remove duplicate URLs within job
- [ ] Job record created in database with status "pending"
- [ ] URLs bulk inserted into database linked to job
- [ ] Response returns: job_id, url_count, duplicates_removed_count
- [ ] Large uploads (10K+ URLs) processed efficiently (<5 seconds)
- [ ] Error handling: invalid file format, no URLs found, file too large (>10MB)

**Story Points:** 3
**Dependencies:** Story 2.1

---

### Story 2.3: Layer 1 Domain Analysis (Pre-Scrape)

**As a** system
**I want to** apply domain and URL pattern analysis WITHOUT making HTTP requests to eliminate 40-60% of candidates before scraping
**So that** we reduce both scraping and LLM costs by eliminating unsuitable domains early

**Acceptance Criteria:**
- [ ] Layer 1 domain analysis service with configurable rules (NO HTTP requests made)
- [ ] Domain classification implemented:
  - Digital-native companies (SaaS, tech startups, agencies) - PASS
  - Traditional companies (manufacturing, retail, hospitality) - REJECT
  - Industry keyword matching (e.g., "software", "consulting", "marketing" vs "restaurant", "hotel", "shop")
- [ ] TLD filtering implemented:
  - Commercial TLDs (.com, .io, .co, .ai) - PASS
  - Non-commercial TLDs (.gov, .edu, .org) - REJECT
  - Personal blog TLDs (.me, .blog, .xyz) - REJECT
- [ ] URL pattern exclusions implemented:
  - Subdomain blogs (blog.example.com, news.example.com) - REJECT
  - Tag/category pages (/tag/, /category/, /author/) - REJECT
  - User-generated content pages (/user/, /profile/) - REJECT
- [ ] Target profile matching implemented:
  - Blog infrastructure indicators in domain name (e.g., "insights", "resources", "learn")
  - Company type signals (presence of "app", "platform", "software" in domain)
- [ ] 40-60% elimination rate achieved and tracked
- [ ] Each elimination logged with reasoning: "REJECT Layer 1 - Non-commercial TLD (.org)"
- [ ] URLs passing Layer 1 marked: "PASS Layer 1 - Proceeding to homepage scraping"
- [ ] Layer 1 analysis executes in <50ms per URL (no network calls)
- [ ] Filter decisions stored with `elimination_layer: 'layer1'` and `layer1_reasoning` fields
- [ ] Configuration loaded from Story 3.0 settings (`layer1_rules` section)
- [ ] Metrics tracked: Layer 1 elimination rate, estimated scraping + LLM cost savings
- [ ] "No HTTP requests made during Layer 1 analysis" requirement enforced

**Story Points:** 3 (refactoring effort: 2-3 days)
**Dependencies:** Story 2.2
**Status:** âœ… Story complete (Epic 2) - requires refactoring for 3-tier architecture (Week 14)

---

### Story 2.6: Layer 2 - Operational Validation Filter (NEW)

**As a** system
**I want to** scrape homepage only and validate company operational readiness
**So that** we eliminate non-operational sites before expensive LLM classification

**Acceptance Criteria:**
- [ ] Layer 2 operational validation service
- [ ] Homepage-only scraping (1 page per domain via ScrapingBee)
- [ ] Company infrastructure signals detection:
  - Active blog detection (recent posts in last 6 months)
  - Technology stack signals (WordPress, modern CMS presence)
  - Required pages existence (/blog, /resources, /about, /contact)
  - Content freshness indicators (copyright year, recent dates)
- [ ] Validation logic:
  - PASS: Company site + active blog + recent content
  - ELIMINATE: Personal blogs, abandoned sites, no blog section
- [ ] Each elimination logged: "ELIMINATE - Layer 2: No active blog detected"
- [ ] Survivors logged: "PASS - Layer 2: Operational site, proceeding to Layer 3"
- [ ] Layer 2 executes in <5 seconds per URL (1 HTTP request)
- [ ] Results stored with `elimination_layer = 'layer2'` and `layer2_signals` JSONB
- [ ] Configurable via classification_settings.layer2_rules
- [ ] Metrics tracked: Layer 2 pass rate, scraping costs saved
- [ ] Target: 70% pass rate (30% elimination)

**Story Points:** 4
**Dependencies:** Story 2.3 (Layer 1 must complete first)

---

### Story 2.4: Layer 3 - LLM Classification with Confidence Scoring (REFACTORED)

**As a** system
**I want to** classify URLs using Gemini primary and GPT fallback with confidence-based routing
**So that** we get reliable classifications at lowest cost and route uncertain results to manual review

**Description:**
Layer 3 of the 3-tier progressive filtering architecture. Performs deep content analysis with confidence scoring (0-1 scale). Classification results include confidence-based routing: high confidence (0.8-1.0) auto-approved, medium/low confidence (0.3-0.79) routed to manual review queue, very low (<0.3) auto-rejected.

**Acceptance Criteria:**

**LLM Service Configuration:**
- [ ] LLM service configured with:
  - Primary: Google Gemini 2.0 Flash API
  - Fallback: OpenAI GPT-4o-mini API
- [ ] Gemini API called first for each URL
- [ ] GPT fallback triggered on: Gemini API error, timeout (>30s), rate limit
- [ ] Fallback logged: "GPT fallback used - Gemini timeout"
- [ ] Retry logic: 3 attempts with exponential backoff (1s, 2s, 4s) for transient errors
- [ ] Permanent failures marked: status "failed", error message stored

**Enhanced Classification Prompt:**
- [ ] Classification prompt includes:
  - Content marketing sophistication indicators (author bylines, editorial quality, audience engagement signals)
  - SEO investment signal detection (structured data, meta optimization, technical SEO implementation)
  - Guest post opportunity signals (contributor sections, guest post guidelines, "write for us" pages)
  - More nuanced reasoning for confidence scoring
- [ ] Prompt requests JSON response: {suitable: boolean, confidence: 0-1, reasoning: string, sophistication_signals: array}

**Confidence Scoring:**
- [ ] Classification response includes `confidence` field (0-1 scale)
- [ ] Confidence reflects LLM certainty in classification decision
- [ ] Confidence scoring considers: signal strength, content clarity, consistency across indicators

**Confidence Bands:**
- [ ] High confidence (0.8-1.0): Auto-approve as "suitable"
- [ ] Medium confidence (0.5-0.79): Route to manual review queue
- [ ] Low confidence (0.3-0.49): Route to manual review queue
- [ ] Auto-reject (0-0.29): Mark as "not_suitable"
- [ ] Store `confidence_band` field (high/medium/low/auto_reject) in database

**Manual Review Queue Routing:**
- [ ] Create `manual-review-router.service.ts`
- [ ] Mark `manual_review_required = true` for medium/low confidence results
- [ ] Track manual review queue size in job metrics
- [ ] Log routing decisions: "Medium confidence (0.65) - Routed to manual review"

**Result Storage:**
- [ ] Classification result stored: classification (SUITABLE/NOT_SUITABLE), confidence score, confidence_band, reasoning, provider used, manual_review_required
- [ ] Cost calculated and stored per URL (based on token usage)
- [ ] Processing time tracked per URL

**Story Points:** 5 (2-3 days refactoring)
**Dependencies:** Story 2.3 (Layer 1), Story 2.6 (Layer 2)
**Status:** Story complete (Epic 2) - requires refactoring for confidence scoring and manual review routing (Week 15)

---

### Story 2.5: 3-Tier Pipeline Orchestration & Real-Time Updates (REFACTORED)

**As a** system
**I want to** process URLs through 3-tier progressive filtering with real-time updates
**So that** dashboard shows per-layer progress and cost optimization results

**Acceptance Criteria:**
- [ ] BullMQ worker configured to process jobs from queue
- [ ] Worker concurrency: 5 concurrent URLs (respects ScrapingBee rate limits)
- [ ] 3-tier progressive filtering flow per URL:
  1. **Layer 1:** Domain analysis (no HTTP) â†’ Eliminate or proceed
  2. **Layer 2:** Homepage scraping + operational validation â†’ Eliminate or proceed
  3. **Layer 3:** LLM classification + confidence routing â†’ Final classification or manual review
- [ ] Processing flow details:
  - Layer 1: Execute domain analysis, log result, STOP if eliminated
  - Layer 2: If Layer 1 PASS â†’ Scrape homepage via ScrapingBee, validate operational signals, STOP if eliminated
  - Layer 3: If Layer 2 PASS â†’ Full site scraping, LLM classification with confidence scoring, route to manual review if medium/low confidence
- [ ] Database updates trigger Supabase Realtime events (dashboard listens)
- [ ] Job status auto-updates: "pending" â†’ "processing_layer1" â†’ "processing_layer2" â†’ "processing_layer3" â†’ "completed"
- [ ] Real-time job metrics updated:
  - `current_layer` (1/2/3)
  - `layer1_eliminated_count`
  - `layer2_eliminated_count`
  - `scraping_cost` (Layer 2 + Layer 3 ScrapingBee costs)
  - `estimated_savings` (Layer 1 + Layer 2 eliminations Ã— avg costs)
- [ ] Per-layer log entries: "Layer 1: Eliminated 450/1000 (45%)", "Layer 2: Eliminated 165/550 (30%)"
- [ ] Pause/resume support: check job status before processing next URL
- [ ] Graceful shutdown: finish current URL's layer before stopping
- [ ] Error handling: failed URLs don't stop job, logged with details
- [ ] ScrapingBee rate limit handling: 429 error â†’ pause 30s, retry
- [ ] Job completion: status "completed", completion timestamp, summary stats including per-layer metrics and cost savings

**Story Points:** 5 (refactoring)
**Dependencies:** Story 2.4 (Layer 3 must be implemented first)
**Effort Estimate:** 2 days refactoring (was 5 days)
**Note:** Story complete (Epic 2) - requires refactoring for 3-tier progressive orchestration (Week 16)

---

## Epic 3: Local Testing & Production Deployment

**Epic Goal:** Enable user configuration of layer-specific classification parameters, validate complete 3-tier system functionality through comprehensive local end-to-end testing with real external APIs, then deploy to Railway production environment with proper configuration, monitoring, and production validation.

**Priority:** P0 (Must Have - blocks production launch)
**Timeline:** Weeks 13-17 (after Epic 2 refactoring completion)
**Story Count:** 4 stories
**Story Points:** ~17 points

**Why This Epic Matters:**
MVP implementation (Epic 1 & 2) requires refactoring for 3-tier architecture and configuration UI adjustments. This epic includes Story 3.0 UI refactoring for layer-specific settings and Story 3.1 comprehensive testing for 3-tier progressive filtering validation. Ensures the system works end-to-end with actual APIs (ScrapingBee, Gemini, GPT, Supabase Realtime) and validates 3-tier architecture performance targets before production deployment.

**Technical Foundation:**
- Local testing with real API credentials
- Chrome DevTools MCP for UI testing and validation
- Railway MCP for automated deployment
- Supabase MCP for database validation
- Production environment configuration and secrets management
- Health checks and monitoring setup
- 3-tier progressive filtering validation

---

### Story 3.0: Classification Settings Management

**As a** team member
**I want to** configure classification parameters through a settings UI
**So that** I can optimize pre-filtering and LLM classification without code changes

**Acceptance Criteria:**

**Backend - Settings Persistence:**
- [ ] Database table created: `classification_settings` with fields:
  - id (UUID, primary key)
  - prefilter_rules (JSONB) - array of {category, pattern, reasoning, enabled}
  - classification_indicators (JSONB) - array of indicator strings
  - llm_temperature (decimal, 0-1, default 0.3)
  - confidence_threshold (decimal, 0-1, default 0.0)
  - content_truncation_limit (integer, default 10000)
  - updated_at (timestamp)
- [ ] GET /api/settings endpoint returns current settings (with defaults if none exist)
- [ ] PUT /api/settings endpoint updates settings with validation
- [ ] Settings validation: regex patterns checked with safe-regex, temperature/confidence 0-1 range
- [ ] Migration created to seed default settings from current hardcoded values

**Backend - Service Integration:**
- [ ] PreFilterService refactored to load rules from database (fallback to defaults if DB unavailable)
- [ ] LLMService refactored to build prompt from database indicators (fallback to defaults)
- [ ] LLMService uses temperature from settings
- [ ] Classification results filtered by confidence_threshold setting
- [ ] Settings cached in-memory with TTL, refreshed on PUT

**Frontend - Settings UI:**
- [ ] Settings page accessible from dashboard navigation ("Settings" link in header)
- [ ] Form sections: (1) Pre-filter Rules, (2) Classification Indicators, (3) LLM Parameters, (4) Confidence Threshold
- [ ] Pre-filter rules: Expandable list with enable/disable toggles, edit pattern/reasoning, add new rule, delete rule
- [ ] Classification indicators: Multi-line textarea with one indicator per line
- [ ] LLM parameters: Temperature slider (0-1, step 0.1), content limit input (1000-50000)
- [ ] Confidence threshold: Slider (0-1, step 0.05) with explanation text
- [ ] "Save Settings" button with optimistic UI update
- [ ] "Reset to Defaults" button with confirmation dialog
- [ ] Form validation: Invalid regex shows error, temperature/confidence range validated
- [ ] Success/error toast notifications on save

**Testing:**
- [ ] Unit tests: Settings service CRUD operations
- [ ] Integration tests: Services use database settings correctly
- [ ] E2E test: Update settings via UI, create job, verify new settings applied to classification
- [ ] Test fallback behavior: Settings service unavailable â†’ uses hardcoded defaults

**Story Points:** 5
**Dependencies:** Story 2.5 complete (requires existing classification services)

---

### Story 3.1: Local End-to-End Testing with Real APIs

**As a** developer
**I want to** test the complete 3-tier progressive filtering system locally with real external APIs, validating Layer 1 domain analysis, Layer 2 homepage scraping, Layer 3 LLM classification with confidence routing, and manual review queue functionality
**So that** I can verify all integrations work before deploying to production

**Acceptance Criteria:**

**AC1: Layer 1 Domain Analysis Testing**
- [ ] Test dataset: 100 URLs spanning all categories (digital-native B2B, traditional companies, blog platforms, social media, forums)
- [ ] Expected: 40-60% eliminated at Layer 1
- [ ] Validate: Domain classification accuracy, TLD filtering, URL pattern exclusions
- [ ] Verify: NO HTTP requests made for Layer 1 eliminations (cost savings confirmed)
- [ ] Confirm: `elimination_layer = 'layer1'` stored correctly

**AC2: Layer 2 Operational Validation Testing**
- [ ] Test dataset: 40-60 URLs passing Layer 1 (Layer 1 survivors)
- [ ] Expected: ~30% eliminated at Layer 2 (70% pass rate)
- [ ] Validate: Homepage scraping (not full site), company infrastructure signal detection, blog freshness validation
- [ ] Verify: Only homepage scraped (ScrapingBee cost tracking accurate)
- [ ] Confirm: `layer2_signals` JSONB populated, `elimination_layer = 'layer2'` if rejected

**AC3: Layer 3 Confidence Distribution Testing**
- [ ] Test dataset: 30-40 URLs passing Layer 2
- [ ] Expected distribution:
  - High confidence (0.8-1.0): 60% â†’ Auto-approved as "suitable"
  - Medium confidence (0.5-0.79): 20% â†’ Routed to manual review queue
  - Low confidence (0.3-0.49): 15% â†’ Routed to manual review queue
  - Auto-reject (0-0.29): 5% â†’ Marked "not_suitable"
- [ ] Validate: LLM confidence scoring, manual review routing logic
- [ ] Verify: Gemini primary / GPT fallback, cost tracking per provider
- [ ] Confirm: `confidence_band` and `manual_review_required` fields correct

**AC4: End-to-End Pipeline Testing**
- [ ] Test dataset: 20 real URLs with known expected outcomes
- [ ] Validate: Complete pipeline flow Layer 1 â†’ Layer 2 â†’ Layer 3
- [ ] Verify: Real-time dashboard updates show `current_layer`, per-layer logs, elimination reasoning
- [ ] Test: Job controls (pause/resume) work correctly during each layer
- [ ] Confirm: Progressive elimination (URLs skip subsequent layers when eliminated)

**AC5: Cost Optimization Validation**
- [ ] Calculate: LLM cost savings (target: 60-70% reduction vs V1)
- [ ] Calculate: Scraping cost savings (target: 40-60% reduction via Layer 1 elimination)
- [ ] Verify: Cost tracking displays per-layer costs (`scraping_cost`, `gemini_cost`, `gpt_cost`)
- [ ] Verify: `estimated_savings` field shows avoided costs from Layer 1 elimination
- [ ] Confirm: Meets NFR003 cost efficiency targets

**AC6: Manual Review Queue Testing**
- [ ] Validate: Medium/low confidence results routed to queue correctly
- [ ] Test: GET `/jobs/:id/manual-review` returns queue entries
- [ ] Test: PATCH `/results/:id/manual-decision` updates classification
- [ ] Verify: Manual decision updates propagate to results table
- [ ] Confirm: Queue size tracking accurate

**AC7: Settings Configuration Testing (3-Tier)**
- [ ] Test: Update Layer 1 rules via Story 3.0 UI â†’ Create job â†’ Verify Layer 1 applies new rules
- [ ] Test: Update Layer 2 thresholds â†’ Verify operational filter uses new thresholds
- [ ] Test: Update confidence bands â†’ Verify Layer 3 routing changes accordingly
- [ ] Validate: Configuration changes persist across job restarts
- [ ] Confirm: Layer-specific settings load correctly in each service

**AC8: Chrome DevTools MCP Validation**
- [ ] Navigate to Settings UI â†’ Verify layer-specific sections (Layer 1/2/3) render
- [ ] Update Layer 1 domain patterns â†’ Save â†’ Verify persistence
- [ ] Create job â†’ Monitor dashboard real-time updates
- [ ] Verify: Logs show per-layer decisions, `current_layer` updates, elimination reasoning per layer
- [ ] Screenshot: Dashboard showing 3-tier progress metrics (layer1_eliminated_count, layer2_eliminated_count)

**AC9: Supabase MCP Validation**
- [ ] Query `classification_settings` â†’ Verify layer-structured schema (layer1_rules, layer2_rules, layer3_rules)
- [ ] Query `results` â†’ Verify new fields (`elimination_layer`, `confidence_band`, `manual_review_required`, `layer1_reasoning`, `layer2_signals`)
- [ ] Query `jobs` â†’ Verify new counters (`layer1_eliminated_count`, `layer2_eliminated_count`, `scraping_cost`, `estimated_savings`, `current_layer`)
- [ ] Verify: Realtime events fire for layer transitions

**AC10: Production Deployment Preparation**
- [ ] All 9 test scenarios above passing
- [ ] Chrome DevTools validation complete
- [ ] Supabase validation complete
- [ ] Performance targets met (Layer 1: 100+ URLs/min, Layer 2: 20-30 URLs/min, Layer 3: 10-15 URLs/min)
- [ ] Cost optimization targets met (60-70% LLM, 40-60% scraping)

**Story Points:** 5
**Dependencies:** Story 2.5 complete

**Note:** Test scenarios completely rewritten for 3-tier progressive filtering architecture (Week 17). Original V1 test scenarios no longer applicable.

---

- [ ] All acceptance criteria from Epic 1 & 2 validated end-to-end in local environment

**Story Points:** 5
**Dependencies:** Story 2.5 complete

---

### Story 3.2: Railway Production Deployment & Configuration

**As a** developer
**I want to** deploy the application to Railway production environment
**So that** the team can use the system for actual URL classification work

**Acceptance Criteria:**
- [ ] Railway project created and linked to GitHub repository
- [ ] Railway services configured: NestJS API, Redis (managed), Frontend (if applicable)
- [ ] Supabase production database configured with proper schema (migrations applied)
- [ ] Environment variables configured in Railway:
  - SCRAPINGBEE_API_KEY (production credits)
  - GEMINI_API_KEY (production quota)
  - OPENAI_API_KEY (production tier)
  - REDIS_URL (Railway managed Redis)
  - DATABASE_URL (Supabase production)
  - SUPABASE_URL, SUPABASE_SERVICE_KEY
- [ ] Railway auto-deploy configured: git push to main triggers deployment
- [ ] Build succeeds in Railway environment (nixpacks.toml configuration verified)
- [ ] Health check endpoint accessible: GET /health returns 200
- [ ] Application starts successfully with all services connected (database, Redis, Supabase)
- [ ] Environment validation runs at startup: fails fast if required env vars missing
- [ ] Railway logs accessible and structured (Pino logger output in JSON format)
- [ ] Domain generated for API access (Railway provided domain or custom domain)
- [ ] CORS configured for production domain
- [ ] Graceful shutdown tested: Railway SIGTERM handling verified (deploys don't interrupt mid-processing)

**Story Points:** 4
**Dependencies:** Story 3.1 complete

---

### Story 3.3: Production Validation & Monitoring Setup

**As a** developer
**I want to** validate production deployment and set up monitoring
**So that** I can ensure system reliability and quickly identify issues

**Acceptance Criteria:**
- [ ] Production smoke test: Create job with 5 URLs, verify complete processing end-to-end
- [ ] Supabase Realtime validated in production: events firing correctly to dashboard
- [ ] Dashboard tested in production: all real-time features working (progress, logs, cost tracking)
- [ ] ScrapingBee production API verified: successful fetches, rate limits respected
- [ ] Gemini production API tested: classifications working, costs tracked accurately
- [ ] GPT fallback verified in production environment
- [ ] Worker concurrency performing as expected: 20 URLs/min target met
- [ ] Database performance validated: <200ms insert/update latency
- [ ] Railway metrics reviewed: memory usage stable (<512MB), CPU usage normal
- [ ] Railway logs validated: structured logs accessible, no critical errors
- [ ] Error scenarios tested: API failures handled gracefully, retry logic working
- [ ] Job pause/resume tested in production environment
- [ ] Cost tracking validated: real production costs match projections
- [ ] Health check endpoint monitored (set up uptime monitoring if needed)
- [ ] Production deployment runbook documented (deployment steps, rollback procedure, common issues)
- [ ] Team access verified: multiple users can access dashboard simultaneously and see same real-time state

**Story Points:** 3
**Dependencies:** Story 3.2 complete

---

## Out of Scope (Phase 2)

Features explicitly deferred to future phases:

**Phase 2 Features:**
- Scheduled jobs (cron-based execution)
- API access (REST API for external tools)
- Advanced filtering (ML-based pre-filtering)
- Email/webhook notifications on completion
- User authentication and multi-tenancy (separate workspaces)
- Excel export format (CSV and JSON are MVP)
- Bulk edit URLs (remove, re-process selected URLs)
- Custom classification prompts (currently fixed prompt)
- Historical job comparison ("Compare Job A vs Job B")
- Advanced analytics dashboard (charts, trends over time)

---

## Epic Sequencing & Timeline

**Weeks 1-2:** Epic 1 Stories 1.1, 1.2, 1.3 (Dashboard foundation + progress tracking)
**Weeks 3-4:** Epic 2 Stories 2.1, 2.2 (Backend setup + URL upload)
**Weeks 5-6:** Epic 1 Stories 1.4, 1.5, 1.6 (Logs, costs, results table) + Epic 2 Story 2.3 (Pre-filtering)
**Weeks 7-8:** Epic 2 Stories 2.4, 2.5 (LLM classification + worker processing)
**Weeks 9-10:** Epic 1 Story 1.7 (Job controls) + Integration testing
**Weeks 11-12:** Bug fixes, polish, final Epic 2 integration testing âœ… COMPLETE
**Week 13:** Epic 3 Story 3.1 (Local E2E testing with real APIs) ðŸ”„ NEXT
**Week 14:** Epic 3 Stories 3.2, 3.3 (Railway deployment + production validation)

**Total Estimated Effort:** 51 story points (~12-14 weeks for solo developer with AI assistance)

---

## Success Criteria

**MVP is considered successful when:**
- âœ… Team can upload 5K+ URLs and start processing (Epic 2 âœ…)
- âœ… Dashboard shows real-time progress with <1s latency (Epic 1 âœ…)
- âœ… Live logs stream all processing activities (Epic 1 âœ…)
- âœ… Multiple team members can view same job simultaneously (Epic 1 âœ…)
- âœ… LLM costs reduced by 40%+ through pre-filtering (Epic 2 âœ…)
- âœ… Jobs complete reliably with <5% failure rate (Epic 2 âœ…)
- âœ… Results exportable to CSV/JSON (Epic 1 âœ…)
- [ ] Local E2E testing passes with real APIs (ScrapingBee, Gemini, GPT) (Epic 3 ðŸ”„)
- [ ] Application deployed successfully to Railway production (Epic 3 ðŸ”„)
- [ ] Production validation complete: 5+ URL batch processed successfully (Epic 3 ðŸ”„)
- [ ] Monitoring and health checks operational (Epic 3 ðŸ”„)
- [ ] Team can access production dashboard and create jobs (Epic 3 ðŸ”„)

---

_This epic breakdown provides clear implementation path for Level 2 project, emphasizing UI/UX transparency while building production-grade backend._
