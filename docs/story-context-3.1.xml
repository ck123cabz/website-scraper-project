<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>3</epicId>
    <storyId>3.1</storyId>
    <title>Local End-to-End Testing with Real APIs</title>
    <status>Draft</status>
    <generatedAt>2025-10-15</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>/Users/s0mebody/Desktop/dev/projects/website-scraper-project/docs/stories/story-3.1.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>developer</asA>
    <iWant>to test the complete system locally with real external APIs (ScrapingBee, Gemini, GPT, Supabase Cloud)</iWant>
    <soThat>I can verify all integrations work end-to-end before deploying to Railway production</soThat>
    <tasks>
- Task 1: Configure Real API Credentials (AC: 1) - 7 subtasks
  - Copy .env.example to .env, add production API keys (SCRAPINGBEE_API_KEY, GEMINI_API_KEY, OPENAI_API_KEY)
  - Verify Supabase Cloud credentials (DATABASE_URL, SUPABASE_URL, SUPABASE_ANON_KEY)
  - Configure Redis URL (local redis://localhost:6379 or Railway managed)
  - Set USE_MOCK_SERVICES=false or remove flag entirely

- Task 2: Start Local Development Environment (AC: 2) - 8 subtasks
  - Start Redis server, backend API (npm run dev), frontend (npm run dev)
  - Verify health endpoint, Bull Board accessible, real services initialized

- Task 3: Create Test Dataset (AC: 3) - 3 subtasks
  - Prepare 10-20 real URLs spanning site types (blogs, news, e-commerce, social media)
  - Save to docs/test-data/e2e-test-urls.txt
  - Document expected outcomes

- Task 4: Execute Test Job via Dashboard (AC: 4, 5, 6, 7) - 10 subtasks
  - Create job via dashboard, paste test URLs, start processing
  - Monitor backend logs for ScrapingBee, Gemini/GPT API calls
  - Verify pre-filter rejections (~40-60% of URLs)
  - Check for GPT fallback usage

- Task 5: Monitor Real-Time Dashboard Updates (AC: 8, 9, 13) - 9 subtasks
  - Watch progress bar, current URL panel, activity log stream
  - Verify cost tracker updates, processing rate displayed
  - Check WebSocket connection to Supabase Cloud
  - Take screenshot using Chrome DevTools MCP
  - Verify NO console errors

- Task 6: Test Job Controls (AC: 10) - 10 subtasks
  - Pause job mid-processing, verify status changes
  - Refresh browser to verify paused state persists
  - Resume job, verify processing continues from last URL

- Task 7: Validate Results and Cost Tracking (AC: 11, 12) - 9 subtasks
  - Review completion summary, check results table
  - Verify cost breakdown (Gemini + GPT = Total)
  - Verify pre-filter savings (rejected URLs = $0 LLM cost)
  - Review activity logs for errors

- Task 8: Test Error Scenarios (AC: 12) - 6 subtasks
  - Add invalid URL, verify error handling
  - Monitor rate limit handling (429 from ScrapingBee)
  - Verify auto-retry logic
  - Check Gemini timeout → GPT fallback

- Task 9: Validate Epic 1 &amp; 2 Acceptance Criteria (AC: 14) - 12 subtasks
  - Verify all FR001-FR012 functional requirements end-to-end
  - Test results export (CSV/JSON download)
  - Verify job history accessible

- Task 10: Document E2E Test Results (AC: 15) - 7 subtasks
  - Create test report: docs/e2e-test-results-[date].md
  - Document job completion summary with metrics
  - Screenshot evidence, performance metrics
  - Recommendations for production deployment
    </tasks>
  </story>

  <acceptanceCriteria>
1. Environment configured with real API credentials
   - SCRAPINGBEE_API_KEY, GEMINI_API_KEY, OPENAI_API_KEY configured in apps/api/.env
   - Supabase Cloud connection working (DATABASE_URL, SUPABASE_URL, SUPABASE_ANON_KEY)
   - Redis connection working (local or Railway managed)

2. Local development environment running
   - Backend API: http://localhost:3001, Frontend: http://localhost:3000
   - Redis server running, BullMQ queue operational
   - Health check passing: curl http://localhost:3001/health

3. Test job created with 10-20 real URLs spanning different site types
   - URLs include: blogs, news sites, e-commerce, social media, forums
   - Job created via dashboard UI or API: POST /jobs/create

4. Worker processes URLs successfully with real external APIs
   - ScrapingBee API calls successful (HTML fetching with JS rendering)
   - Pre-filter rules applied (regex-based rejection of obvious non-targets)
   - LLM classification executes (Gemini API calls with structured responses)
   - Results stored in Supabase Cloud database
   - No mock services used (USE_MOCK_SERVICES=false)

5. Gemini primary usage verified
   - Logs show "Gemini classification" for majority of URLs
   - Gemini API responses valid (suitable/not_suitable with score and reasoning)
   - Gemini costs tracked correctly (~$0.0004 per classification)

6. GPT fallback tested
   - Trigger fallback scenario (Gemini timeout, rate limit, or API error)
   - Logs show "GPT fallback used" with reason
   - GPT classification successful, costs tracked (~$0.0012 per classification)

7. Pre-filter correctly rejects known platforms
   - 40-60% of URLs rejected by pre-filter (no LLM call)
   - Logs show "REJECT - Blog platform" or similar reasoning
   - Rejected URLs marked as rejected_prefilter in results table
   - Cost savings validated (rejected URLs = $0 LLM cost)

8. Supabase Realtime events firing (Supabase Cloud)
   - Job progress updates in real-time (&lt;1s latency)
   - Activity logs stream to dashboard
   - Results table updates as URLs processed
   - Supabase Realtime WebSocket connection stable

9. Dashboard updates in real-time
   - Progress bar, counters, processing rate update live
   - Live activity log scrolls automatically
   - Cost tracker updates: total cost, Gemini vs GPT breakdown

10. Job controls tested (pause/resume) with state persistence
    - Pause job mid-processing, status changes to "paused"
    - Worker stops processing new URLs (current URL completes)
    - Resume job, processing continues from last URL
    - Refresh browser - paused state persists

11. Cost tracking validated
    - Total cost calculated correctly (sum of all URL costs)
    - Gemini vs GPT cost breakdown accurate
    - Projected total cost calculated (remaining URLs × avg cost)

12. Error handling tested (API failures, timeouts, rate limits)
    - ScrapingBee rate limit (429) - auto-retry after delay
    - Gemini API timeout - GPT fallback triggered
    - Invalid URL - marked as failed with error message
    - Failed URLs don't crash job

13. Chrome DevTools MCP used to verify UI updates
    - Take screenshot of dashboard during processing
    - Verify real-time progress updates in browser
    - Check Network tab for API calls
    - Verify WebSocket connection to Supabase Cloud

14. All Epic 1 &amp; 2 acceptance criteria validated end-to-end
    - Epic 1: Real-time dashboard features working (FR001-FR006)
    - Epic 2: Processing pipeline features working (FR007-FR011)
    - Results exportable, job history accessible

15. Local E2E test completion summary
    - 10-20 URLs processed successfully (&gt;95% success rate)
    - Processing time reasonable (~20 URLs/min target)
    - Total cost &lt; $0.50 for test run
    - System stable and ready for production deployment
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/docs/PRD.md" title="Product Requirements Document" section="FR001-FR012, NFR001-NFR005">
        Complete functional and non-functional requirements for the website scraper platform. Key requirements: FR001 (Live Job Dashboard), FR002 (Current URL Display), FR003 (Live Activity Logs), FR004 (Historical Results), FR005 (Real-Time Progress), FR006 (Cost Tracking), FR009 (AI Classification), FR010 (Job Controls). Non-functional: NFR001 (Real-time UI &lt;500ms), NFR002 (20 URLs/min processing), NFR003 (Cost efficiency), NFR004 (95% uptime, error handling).
      </doc>
      <doc path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/docs/tech-spec-epic-2.md" title="Epic 2 Technical Specification" section="Processing Pipeline Architecture">
        Complete backend processing pipeline specification: NestJS + BullMQ + Redis architecture, ScrapingBee integration (30s timeout, 10 req/sec limit), LLM service (Gemini 2.0 Flash primary at $0.0003/1K input, GPT-4o-mini fallback at $0.0005/1K input), pre-filtering engine (40-60% rejection target), worker processing with 5 concurrent URLs, automatic retry logic (3 attempts, exponential backoff), real-time database updates triggering Supabase Realtime events.
      </doc>
      <doc path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/docs/epic-stories.md" title="Epic Stories" section="Story 3.1">
        Original Story 3.1 specification with acceptance criteria and dependencies. This story depends on Epic 1 (Stories 1.1-1.7) and Epic 2 (Stories 2.1-2.5) completion. Success criteria: validate all integrations, processing rate 15-25 URLs/min with real APIs, cost per URL $0.0004-0.0012, real-time dashboard updates &lt;1s latency, &gt;95% success rate for valid URLs.
      </doc>
      <doc path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/docs/architecture-summary.md" title="Architecture Summary" section="System Overview">
        High-level system architecture: Monorepo structure (Turborepo), apps/api (NestJS backend), apps/web (Next.js 14 frontend), Supabase PostgreSQL database with Realtime, Redis for BullMQ, Railway deployment platform. Integration patterns for ScrapingBee, Gemini, GPT APIs. Database schema: jobs, results, activity_logs tables.
      </doc>
    </docs>
    <code>
      <artifact path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/api/src/workers/url-worker.processor.ts" kind="service" symbol="UrlWorkerProcessor" lines="1-665" reason="Core BullMQ worker processing pipeline implementing: fetch URL (ScrapingBee), extract content (cheerio), pre-filter (regex rules), classify (Gemini/GPT), store result (Supabase), update job counters (atomic increments via increment_job_counters RPC), log activity, trigger Realtime events. Implements concurrency=5, retry logic with exponential backoff, pause/resume support via job status checks, graceful shutdown handling. This is the primary component to validate in E2E testing." />
      <artifact path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/api/src/jobs/services/llm.service.ts" kind="service" symbol="LlmService" lines="1-409" reason="LLM classification service implementing Gemini 2.0 Flash primary (gemini-2.0-flash-exp model) with GPT-4o-mini fallback. Key features: timeout handling (30s), automatic fallback on Gemini failure, retry logic with exponential backoff (3 attempts, 1s/2s/4s delays), cost calculation (Gemini: $0.0003/1K input + $0.0015/1K output, GPT: $0.0005/1K input + $0.002/1K output), JSON response parsing with validation. Must test: Gemini success path, GPT fallback trigger, cost tracking accuracy, timeout handling." />
      <artifact path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/api/src/scraper/scraper.service.ts" kind="service" symbol="ScraperService" lines="1-347" reason="ScrapingBee API integration service for web scraping with JS rendering. Key features: 30s timeout, HTML fetching via GET https://app.scrapingbee.com/api/v1/, content extraction with cheerio (title, meta description, body text), comprehensive error handling (429 rate limit, timeouts, auth errors), retry logic for transient errors. Must test: successful fetch, content extraction, 429 handling, timeout behavior, error messages in logs." />
      <artifact path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/api/src/jobs/services/prefilter.service.ts" kind="service" symbol="PreFilterService" lines="1-200" reason="Intelligent pre-filtering service implementing regex-based URL rejection before LLM classification. Target: 40-60% rejection rate to reduce LLM costs. Rules reject known blog platforms (wordpress.com, medium.com, blogger.com, substack.com), social media (twitter.com, facebook.com, linkedin.com, instagram.com), e-commerce, forums. Returns passed/rejected with reasoning string. Must test: rejection rate on diverse URL set, reasoning logged to activity_logs." />
      <artifact path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/api/src/queue/queue.service.ts" kind="service" symbol="QueueService" lines="1-150" reason="BullMQ queue management service for job dispatching and control. Key operations: addUrlToQueue (dispatches individual URLs to url-processing-queue), pauseJob (updates job status to paused in database), resumeJob (updates status to processing), getQueueStats. Worker checks job status before processing each URL to implement pause/resume. Must test: pause/resume functionality, queue depth monitoring." />
      <artifact path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/api/src/jobs/jobs.controller.ts" kind="controller" symbol="JobsController" lines="1-200" reason="NestJS REST API controller exposing job management endpoints. Key endpoints: POST /jobs/create (accepts CSV/JSON/text with URLs, creates job, dispatches to queue), GET /jobs/:id (fetch job details), PATCH /jobs/:id/pause (pause active job), PATCH /jobs/:id/resume (resume paused job), DELETE /jobs/:id/cancel. Must test: job creation with test URLs, pause/resume via HTTP endpoints, job status updates in database." />
      <artifact path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/api/src/main.ts" kind="entry-point" symbol="bootstrap" lines="1-50" reason="NestJS application entry point with health check endpoint configuration. Initializes BullMQ, Supabase client, validates environment variables (SCRAPINGBEE_API_KEY, GEMINI_API_KEY, OPENAI_API_KEY, DATABASE_URL, REDIS_URL). Health endpoint: GET /health checks database connection, Redis connection, API availability. Must test: health check returns 200, environment variable validation, graceful startup/shutdown." />
      <artifact path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/web/app/dashboard/page.tsx" kind="component" symbol="DashboardPage" lines="1-200" reason="Next.js dashboard page component implementing real-time job monitoring UI. Uses TanStack Query for server state, Supabase Realtime subscriptions for live updates (jobs, results, activity_logs tables). Displays: job cards with progress bars, current URL panel, live activity log, cost tracker, results table. Must test with Chrome DevTools MCP: verify UI updates &lt;1s latency, progress bar animation, WebSocket connection (wss://[project].supabase.co/realtime/v1/websocket)." />
      <artifact path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/web/hooks/use-jobs.ts" kind="hook" symbol="useJobs" lines="1-150" reason="React hook implementing TanStack Query + Supabase Realtime integration for live job updates. Subscribes to postgres_changes events on jobs table, invalidates queries on INSERT/UPDATE/DELETE, provides optimistic updates for pause/resume actions. Must test: Realtime subscription setup, query invalidation on database changes, real-time latency measurement (&lt;500ms target per NFR001)." />
      <artifact path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/web/components/job-card.tsx" kind="component" symbol="JobCard" lines="1-150" reason="Job status card component displaying: job name, status badge, progress bar, counters (processed/total URLs, success/failure), processing rate (URLs/min), time indicators (elapsed, remaining), cost tracker. Updates in real-time via useJobs hook. Must test with Chrome DevTools MCP: verify counters increment, progress bar animates, cost updates live, pause/resume buttons trigger API calls." />
      <artifact path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/web/components/current-url-panel.tsx" kind="component" symbol="CurrentUrlPanel" lines="1-80" reason="Displays currently processing URL with details: URL, site name, current stage (fetching/filtering/classifying), elapsed time for current URL, progress indicator. Updates in real-time via jobs.current_url, jobs.current_stage, jobs.current_url_started_at fields. Must test: updates &lt;1s after database change, stage transitions visible, timer counts up." />
    </code>
    <dependencies>
      <node>
        <package name="@nestjs/core" version="^10.3.0" reason="NestJS framework - backend application server" />
        <package name="@nestjs/bullmq" version="^10.1.0" reason="BullMQ integration for NestJS - job queue processing" />
        <package name="bullmq" version="^5.0.0" reason="Redis-based job queue - worker processing with concurrency=5" />
        <package name="@supabase/supabase-js" version="^2.39.0" reason="Supabase client for PostgreSQL database and Realtime subscriptions" />
        <package name="axios" version="^1.12.2" reason="HTTP client for ScrapingBee, Gemini, GPT API calls" />
        <package name="cheerio" version="^1.1.2" reason="HTML parsing for content extraction from scraped pages" />
        <package name="@google/generative-ai" version="^0.24.1" reason="Google Gemini API client - primary LLM classification provider" />
        <package name="openai" version="^6.3.0" reason="OpenAI GPT API client - fallback LLM classification provider" />
        <package name="next" version="^14.2.15" reason="Next.js 14 - frontend framework with App Router" />
        <package name="react" version="^18.3.0" reason="React 18 - UI library for dashboard components" />
        <package name="@tanstack/react-query" version="^5.90.2" reason="Server state management - handles API calls, caching, real-time invalidation" />
        <package name="@radix-ui/react-progress" version="^1.1.7" reason="Progress bar component for job progress visualization" />
        <package name="lucide-react" version="^0.545.0" reason="Icon library for UI components" />
      </node>
      <external>
        <api name="ScrapingBee API" endpoint="https://app.scrapingbee.com/api/v1/" reason="Web scraping with JS rendering - must test with real API key, verify 30s timeout, 429 rate limit handling, JS rendering enabled (render_js=true parameter)" />
        <api name="Google Gemini 2.0 Flash" endpoint="https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent" reason="Primary LLM classification - must test with real API key, verify timeout handling (30s), cost calculation ($0.0003/1K input + $0.0015/1K output), JSON response parsing" />
        <api name="OpenAI GPT-4o-mini" endpoint="https://api.openai.com/v1/chat/completions" reason="Fallback LLM classification - must test fallback trigger (disable Gemini), cost calculation ($0.0005/1K input + $0.002/1K output), response_format: json_object" />
        <api name="Supabase PostgreSQL" endpoint="DATABASE_URL from Supabase dashboard" reason="Production database - must verify connection from local backend, query jobs/results/activity_logs tables, test atomic increment_job_counters RPC function" />
        <api name="Supabase Realtime" endpoint="wss://[project-ref].supabase.co/realtime/v1/websocket" reason="Real-time events - must verify WebSocket connection from browser, test postgres_changes subscriptions on jobs/results/activity_logs, measure latency (&lt;500ms target)" />
      </external>
      <local>
        <service name="Redis" port="6379" reason="BullMQ job queue storage - run locally with redis-server or use Railway managed Redis URL, must verify connection with redis-cli ping" />
        <service name="NestJS Backend" port="3001" reason="Backend API server - must run with npm run dev, verify health check GET http://localhost:3001/health, check logs for API initialization" />
        <service name="Next.js Frontend" port="3000" reason="Frontend dashboard - must run with npm run dev, open http://localhost:3000, verify Supabase Realtime connection in Network tab" />
      </local>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint type="environment" scope="local-testing">
      Must configure apps/api/.env with real production API keys: SCRAPINGBEE_API_KEY (production credits), GEMINI_API_KEY (Google AI Studio), OPENAI_API_KEY (OpenAI production tier). Also requires Supabase Cloud credentials: DATABASE_URL, SUPABASE_URL, SUPABASE_SERVICE_KEY, SUPABASE_ANON_KEY. Redis URL: redis://localhost:6379 or Railway managed URL. Set USE_MOCK_SERVICES=false or omit entirely to ensure real API usage.
    </constraint>
    <constraint type="infrastructure" scope="local-services">
      Requires local development environment: Backend running on http://localhost:3001, Frontend on http://localhost:3000, Redis on localhost:6379 (or Railway managed). All services must start successfully before creating test job. Health check must return 200 status with database and Redis connection confirmed.
    </constraint>
    <constraint type="testing" scope="test-data">
      Test dataset must include 10-20 diverse real URLs to achieve 40-60% pre-filter rejection rate: 3-4 potential guest post sites (marketing blogs, tech blogs), 3-4 blog platforms (wordpress.com, medium.com, substack.com), 2-3 news sites (techcrunch.com, wired.com), 2-3 e-commerce sites (shopify stores, amazon.com), 2-3 social media profiles (twitter.com, linkedin.com). Save to docs/test-data/e2e-test-urls.txt with expected outcomes documented.
    </constraint>
    <constraint type="performance" scope="validation-targets">
      Must validate processing rate 15-25 URLs/min with real APIs (slower than mocks due to ScrapingBee ~5-15s, Gemini ~2-5s, GPT ~3-8s latencies). Real-time dashboard updates must occur &lt;1s after database change (Supabase Realtime WebSocket). Success rate &gt;95% for valid URLs. Total cost for 10-20 URL test batch must be &lt;$0.50.
    </constraint>
    <constraint type="cost" scope="api-usage">
      Must track and validate cost calculation: Gemini cost ($0.0003/1K input + $0.0015/1K output) vs GPT cost ($0.0005/1K input + $0.002/1K output). Pre-filter rejections must save costs (rejected URLs = $0 LLM cost). Cost per URL target: $0.0004-0.0012 depending on Gemini vs GPT usage. Gemini primary usage should be 70-90%, GPT fallback 10-30%.
    </constraint>
    <constraint type="integration" scope="epic-validation">
      All Epic 1 (Stories 1.1-1.7) and Epic 2 (Stories 2.1-2.5) acceptance criteria must be validated end-to-end with real APIs before production deployment. Must verify: FR001 (Live Job Dashboard), FR002 (Current URL Display), FR003 (Live Activity Logs), FR004 (Historical Results), FR005 (Real-Time Progress), FR006 (Cost Tracking), FR007 (Bulk URL Upload), FR008 (Pre-Filtering), FR009 (AI Classification), FR010 (Job Controls), FR011 (Automatic Retry).
    </constraint>
    <constraint type="testing-tools" scope="mcp-usage">
      Must use Chrome DevTools MCP for UI validation: mcp__chrome-devtools__navigate_page (open dashboard), mcp__chrome-devtools__take_screenshot (capture UI state at 0%, 25%, 50%, 100% progress), mcp__chrome-devtools__take_snapshot (verify UI elements), mcp__chrome-devtools__click (test pause/resume buttons), mcp__chrome-devtools__list_network_requests (verify API calls to localhost:3001, WebSocket to Supabase). Must use Supabase MCP for database validation: mcp__supabase__execute_sql (query job status, results table), mcp__supabase__get_logs (check database logs).
    </constraint>
    <constraint type="testing-philosophy" scope="always-works-testing">
      ALWAYS WORKS™ Testing Requirements: (1) Run actual code (no assumptions), (2) Use real external APIs (ScrapingBee, Gemini, GPT), (3) Connect to real Supabase Cloud database, (4) Click actual buttons in browser (use Chrome DevTools MCP), (5) Verify database changes (use Supabase MCP queries), (6) Monitor backend logs for API calls, (7) Check browser console for errors, (8) Take screenshots as evidence, (9) Document actual costs incurred, (10) Measure actual processing times. Success = answer YES to: Did I create real test job? See ScrapingBee API calls? See Gemini/GPT classifications? Dashboard update in real-time? Pause worked? Verified database? Checked console errors? Would bet $100 this is production-ready?
    </constraint>
  </constraints>

  <interfaces>
    <interface name="Health Check Endpoint" kind="HTTP" signature="GET /health" path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/api/src/main.ts">
      Health check endpoint must return 200 status with JSON body containing database connection status (Supabase), Redis connection status (BullMQ), and API availability (ScrapingBee, Gemini, GPT). Used for: Railway health checks, local development verification before starting test job. Test: curl http://localhost:3001/health
    </interface>
    <interface name="Create Job Endpoint" kind="HTTP" signature="POST /jobs/create" path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/api/src/jobs/jobs.controller.ts">
      Create new scraping job with URL list. Accepts: multipart/form-data (CSV/TXT file upload), application/json ({"urls": ["url1", "url2", ...]}), text/plain (line-separated URLs). Returns: job_id, url_count, duplicates_removed_count. Job status set to "pending", URLs dispatched to BullMQ queue, status updated to "processing" on first worker pickup. Test: POST http://localhost:3001/jobs/create with test URL list.
    </interface>
    <interface name="Pause Job Endpoint" kind="HTTP" signature="PATCH /jobs/:id/pause" path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/api/src/jobs/jobs.controller.ts">
      Pause active scraping job. Updates job status to "paused" in Supabase database. Worker checks job status before processing each URL - if paused, skips URL and acks job. Current URL completes processing before worker stops. Test: Create job, wait until 50% complete, call PATCH http://localhost:3001/jobs/:id/pause, verify worker stops, dashboard shows "Paused" status.
    </interface>
    <interface name="Resume Job Endpoint" kind="HTTP" signature="PATCH /jobs/:id/resume" path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/api/src/jobs/jobs.controller.ts">
      Resume paused scraping job. Updates job status to "processing" in database. Worker resumes processing from last completed URL (no re-processing of completed URLs). Test: Resume paused job, verify dashboard shows "Processing" status, worker continues, processed_urls counter increments.
    </interface>
    <interface name="Supabase Realtime Subscription" kind="WebSocket" signature="supabase.channel('jobs').on('postgres_changes', {event: '*', schema: 'public', table: 'jobs'}, handler)" path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/web/hooks/use-jobs.ts">
      WebSocket subscription to Supabase Realtime for live job updates. Frontend subscribes to postgres_changes events on jobs, results, activity_logs tables. Events fired automatically when worker updates database (INSERT/UPDATE on results, UPDATE on jobs counters, INSERT on activity_logs). Test with Chrome DevTools Network tab: verify WebSocket connection to wss://[project].supabase.co/realtime/v1/websocket, see realtime messages on job progress updates, measure latency (should be &lt;500ms from database write to UI update).
    </interface>
    <interface name="ScrapingBee API" kind="HTTP" signature="GET https://app.scrapingbee.com/api/v1/?api_key=...&amp;url=...&amp;render_js=true" path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/api/src/scraper/scraper.service.ts">
      ScrapingBee web scraping API with JavaScript rendering. Parameters: api_key (from env), url (target URL), render_js=true (enable JS execution), premium_proxy=false (use standard proxies). Returns: HTML content as string. Timeout: 30s. Rate limit: ~10 req/sec (250K credits/month). Error codes: 429 (rate limit - retry after 30s), 401 (auth error), 500 (server error). Test: Verify HTML content returned, content extraction (title, meta description, body text), 429 handling with retry, timeout behavior.
    </interface>
    <interface name="Gemini API" kind="HTTP" signature="POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent" path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/api/src/jobs/services/llm.service.ts">
      Google Gemini 2.0 Flash API for primary LLM classification. Model: gemini-2.0-flash-exp. Input: classification prompt with URL and content (max 10K chars). Output: JSON with {suitable: boolean, confidence: number, reasoning: string}. Timeout: 30s (fallback to GPT on timeout). Cost: $0.0003/1K input tokens + $0.0015/1K output tokens. Test: Verify JSON response parsing, cost calculation from usageMetadata (promptTokenCount, candidatesTokenCount), timeout handling → GPT fallback, classification accuracy.
    </interface>
    <interface name="GPT API" kind="HTTP" signature="POST https://api.openai.com/v1/chat/completions" path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/api/src/jobs/services/llm.service.ts">
      OpenAI GPT-4o-mini API for fallback LLM classification. Model: gpt-4o-mini. Messages: [{role: "system", content: "..."}, {role: "user", content: classification_prompt}]. Parameters: temperature=0.3, response_format={type: "json_object"}. Output: JSON with {suitable: boolean, confidence: number, reasoning: string}. Timeout: 30s. Cost: $0.0005/1K input tokens + $0.002/1K output tokens. Test: Trigger fallback by disabling Gemini, verify GPT called, cost calculation from usage (prompt_tokens, completion_tokens), classification result stored with llm_provider="gpt".
    </interface>
    <interface name="BullMQ Queue" kind="Redis" signature="Queue.add('url-processing-queue', {jobId, url, urlId})" path="/Users/s0mebody/Desktop/dev/projects/website-scraper-project/apps/api/src/queue/queue.service.ts">
      BullMQ job queue for URL processing. Queue name: "url-processing-queue". Job data: {jobId: string, url: string, urlId: string}. Worker concurrency: 5 (processes 5 URLs concurrently). Worker picks jobs FIFO, checks job status before processing (pause/resume support). Test: Add 10 jobs to queue, verify worker picks them up, monitor queue depth via Bull Board (http://localhost:3001/admin/queues), test pause (queue depth stops decreasing), resume (queue depth resumes decreasing).
    </interface>
  </interfaces>

  <tests>
    <standards>
      This is an END-TO-END TESTING story, not a unit/integration testing story. Testing approach: Manual E2E testing with real external APIs (ScrapingBee, Gemini, GPT, Supabase Cloud) from local development environment. Test execution: (1) Configure real API credentials in .env files, (2) Start local services (Redis, Backend, Frontend), (3) Create test job with 10-20 real URLs via dashboard, (4) Monitor backend logs for API calls, (5) Verify dashboard updates in real-time using Chrome DevTools MCP, (6) Query database using Supabase MCP to confirm data persistence, (7) Test job controls (pause/resume), (8) Document results with screenshots and metrics. Testing philosophy: ALWAYS WORKS™ - run actual code, click actual buttons, verify actual database changes, measure actual costs and processing times. No mocks, no assumptions, no shortcuts. Success = system is production-ready based on real-world testing evidence.
    </standards>
    <locations>
      NO NEW TEST FILES CREATED FOR THIS STORY. Validation performed manually with real services:
      - Backend logs: Terminal output from "cd apps/api &amp;&amp; npm run dev" showing ScrapingBee API calls, Gemini/GPT classifications, BullMQ worker processing
      - Browser console: Chrome DevTools Console tab at http://localhost:3000 for React/Next.js errors, warnings
      - Network tab: Chrome DevTools Network tab for API calls to http://localhost:3001 and WebSocket to wss://[project].supabase.co/realtime/v1/websocket
      - Database queries: Supabase MCP execute_sql tool for querying jobs, results, activity_logs tables
      - Test data: docs/test-data/e2e-test-urls.txt (10-20 test URLs with expected outcomes documented)
      - Test report: docs/e2e-test-results-[date].md (job completion summary, metrics, screenshots, issues, recommendations)
      - Screenshots: Captured via Chrome DevTools MCP at 0%, 25%, 50%, 100% progress showing dashboard UI updates
    </locations>
    <ideas>
      <test-scenario id="AC1-2" description="Local Environment Setup and Health Check">
        Goal: Verify local development environment configured correctly with real API credentials before starting test job.
        Steps:
        1. Copy apps/api/.env.example to apps/api/.env, add real API keys: SCRAPINGBEE_API_KEY (production), GEMINI_API_KEY (Google AI Studio), OPENAI_API_KEY (OpenAI), DATABASE_URL (Supabase Cloud), REDIS_URL (redis://localhost:6379 or Railway)
        2. Start Redis: redis-server (or verify Railway Redis connection)
        3. Start backend: cd apps/api &amp;&amp; npm run dev
        4. Verify backend logs show: "Nest application successfully started", "ScrapingBee client initialized", "Gemini client initialized", "OpenAI client initialized", "BullMQ queue connected"
        5. Test health endpoint: curl http://localhost:3001/health - expect 200 status with database, Redis, API checks passing
        6. Start frontend: cd apps/web &amp;&amp; npm run dev
        7. Open browser: http://localhost:3000, verify dashboard loads, no console errors
        8. Verify Bull Board accessible: http://localhost:3001/admin/queues showing url-processing-queue
        Expected: All services start successfully, health check passes, dashboard accessible, real API clients initialized (NOT mock services)
      </test-scenario>
      <test-scenario id="AC3-4" description="Create Test Job with Real URLs and Monitor Processing">
        Goal: Create test job with 10-20 diverse URLs, verify worker processes with real ScrapingBee/Gemini/GPT APIs, monitor backend logs.
        Steps:
        1. Prepare test URL list in docs/test-data/e2e-test-urls.txt with 10-20 URLs: 3-4 potential guest post sites, 3-4 blog platforms (wordpress.com, medium.com), 2-3 news sites, 2-3 e-commerce, 2-3 social media
        2. Open dashboard http://localhost:3000, click "New Job" button
        3. Paste test URLs from e2e-test-urls.txt into textarea
        4. Give job name: "E2E Test - Real APIs - [Date]"
        5. Click "Start Processing", record job ID from URL
        6. Monitor backend terminal logs for: "ScrapingBee API called", "Gemini classification", "GPT fallback used" (if any Gemini failures), "Pre-filter: PASS/REJECT"
        7. Verify logs show real API responses: ScrapingBee HTML content length, Gemini token counts, GPT usage, cost calculations
        8. Check for pre-filter rejections (~40-60% of URLs): "REJECT - Blog platform" logs
        Expected: Job processes all URLs with real API calls (no mocks), ScrapingBee fetches HTML, pre-filter rejects 40-60%, Gemini classifies majority (70-90%), GPT fallback used for 10-30% (if any failures), results stored in database
      </test-scenario>
      <test-scenario id="AC5-6-7" description="Validate Gemini Primary, GPT Fallback, and Pre-Filter">
        Goal: Verify Gemini used as primary LLM (70-90% of URLs), GPT fallback triggered correctly, pre-filter saves costs.
        Steps:
        1. After test job completes, query database: mcp__supabase__execute_sql "SELECT llm_provider, COUNT(*) FROM results WHERE job_id='...' GROUP BY llm_provider"
        2. Verify Gemini count &gt; GPT count (expect Gemini 70-90%, GPT 10-30%)
        3. Query pre-filter results: "SELECT classification_result, COUNT(*) FROM results WHERE job_id='...' GROUP BY classification_result"
        4. Verify rejected_prefilter count ~40-60% of total URLs
        5. Calculate cost savings: rejected_urls × avg_llm_cost_per_url = savings
        6. Review backend logs for GPT fallback triggers: search for "GPT fallback used" messages with reasons (timeout, error, rate limit)
        7. Verify cost calculations: Query "SELECT SUM(llm_cost) FROM results WHERE job_id='...' AND llm_provider='gemini'" and compare to Gemini pricing ($0.0003/1K input + $0.0015/1K output)
        Expected: Gemini primary usage 70-90%, GPT fallback 10-30%, pre-filter rejection 40-60%, cost per URL $0.0004-0.0012, rejected URLs = $0 LLM cost
      </test-scenario>
      <test-scenario id="AC8-9" description="Validate Supabase Realtime and Dashboard Updates">
        Goal: Verify Supabase Realtime WebSocket connection stable, dashboard updates in real-time (&lt;1s latency), all UI components update live.
        Steps:
        1. Open Chrome DevTools (F12) → Network tab, filter: WS (WebSocket)
        2. Verify WebSocket connection to wss://[project].supabase.co/realtime/v1/websocket with status "101 Switching Protocols"
        3. Create new test job, start processing
        4. Watch dashboard UI: Progress bar animates 0% → 100%, counters increment (processed_urls, successful_urls), processing rate displayed (URLs/min)
        5. Verify Current URL Panel updates: Shows current URL, stage (fetching/filtering/classifying), elapsed time for current URL
        6. Verify Live Activity Log scrolls automatically with new entries (&lt;1s latency from backend log timestamp to dashboard display)
        7. Verify Cost Tracker updates: Total cost increments, Gemini vs GPT breakdown updates, projected cost recalculates
        8. Use Chrome DevTools MCP: mcp__chrome-devtools__take_screenshot to capture dashboard at 25%, 50%, 75% progress
        9. Measure Realtime latency: Compare backend log timestamp "Classification complete at 14:32:15.123" to dashboard activity log timestamp (should be &lt;500ms difference)
        Expected: WebSocket stable, dashboard updates &lt;1s latency, all UI components (progress bar, counters, current URL, logs, cost) update in real-time without manual refresh
      </test-scenario>
      <test-scenario id="AC10" description="Test Job Pause/Resume with State Persistence">
        Goal: Verify pause/resume functionality works correctly, state persists in database, worker stops/resumes processing.
        Steps:
        1. Create test job with 15-20 URLs
        2. Wait until job ~50% complete (7-10 URLs processed)
        3. Click "Pause" button in dashboard
        4. Verify dashboard immediately shows "Paused" status
        5. Check backend logs: Worker should complete current URL, then stop processing ("Skipping URL - job paused")
        6. Query database: mcp__supabase__execute_sql "SELECT status FROM jobs WHERE id='...'" - expect status="paused"
        7. Refresh browser (F5), verify paused state persists (dashboard still shows "Paused", same processed_urls count)
        8. Click "Resume" button
        9. Verify dashboard shows "Processing" status, processed_urls counter resumes incrementing
        10. Check backend logs: Worker continues from last processed URL (no re-processing of completed URLs)
        11. Wait for job completion, verify all remaining URLs processed successfully
        Expected: Pause stops worker immediately after current URL, paused state persists in database and browser refresh, resume continues from last URL without re-processing, all URLs eventually processed
      </test-scenario>
      <test-scenario id="AC11-12" description="Validate Cost Tracking and Error Handling">
        Goal: Verify cost tracking accurate (Gemini vs GPT breakdown, projected costs), error handling works (API failures, timeouts, invalid URLs).
        Steps:
        1. After test job completes, review completion summary in dashboard
        2. Verify Total Cost = SUM(llm_cost) from results table: Query database "SELECT SUM(llm_cost) FROM results WHERE job_id='...'"
        3. Verify Gemini Cost + GPT Cost = Total Cost: Query "SELECT SUM(llm_cost) FROM results WHERE job_id='...' AND llm_provider='gemini'" and same for GPT
        4. Calculate cost per URL: total_cost / processed_urls (expect $0.0004-0.0012)
        5. Verify projected cost accuracy: At 50% progress, projected_cost = current_cost × (total_urls / processed_urls). Compare projected vs actual final cost (should be within 10%)
        6. Add invalid URL to new test job (e.g., "not-a-valid-url"), verify error logged to activity_logs, URL marked as "failed" in results table
        7. Monitor for ScrapingBee rate limit (429): If occurs naturally, verify backend logs show "Rate limit exceeded, retrying in 30s", worker pauses 30s, resumes successfully
        8. Check for Gemini timeout → GPT fallback: If Gemini takes &gt;30s (rare), verify logs show "Gemini timeout, falling back to GPT", GPT classification stored
        Expected: Cost breakdown accurate (Gemini + GPT = Total), cost per URL $0.0004-0.0012, projected costs within 10% of actual, errors handled gracefully (invalid URLs fail without crashing job, rate limits trigger retry, timeouts trigger fallback)
      </test-scenario>
      <test-scenario id="AC13-14" description="Chrome DevTools MCP UI Validation and Epic 1+2 Verification">
        Goal: Use Chrome DevTools MCP to verify UI updates, test button clicks, monitor WebSocket, validate all Epic 1 &amp; 2 acceptance criteria end-to-end.
        Steps:
        1. Use mcp__chrome-devtools__navigate_page {url: "http://localhost:3000"} to open dashboard
        2. Use mcp__chrome-devtools__take_snapshot to capture UI elements at job start (0% progress)
        3. Create test job, start processing
        4. Use mcp__chrome-devtools__take_screenshot at 0%, 25%, 50%, 75%, 100% progress to document UI updates
        5. Use mcp__chrome-devtools__list_network_requests {resourceTypes: ["fetch", "xhr", "websocket"]} to verify API calls to http://localhost:3001 and WebSocket to Supabase
        6. Use mcp__chrome-devtools__click to test Pause button, verify UI updates, use click again for Resume button
        7. Verify NO console errors: Use mcp__chrome-devtools__list_console_messages, filter for errors/warnings
        8. Validate Epic 1 criteria: FR001 (Live Job Dashboard), FR002 (Current URL Display), FR003 (Live Activity Logs), FR004 (Historical Results Table), FR005 (Real-Time Progress Indicators), FR006 (Cost Tracking Display)
        9. Validate Epic 2 criteria: FR007 (Bulk URL Upload), FR008 (Intelligent Pre-Filtering 40-60% rejection), FR009 (AI-Powered Classification with Gemini/GPT), FR010 (Job Control Actions pause/resume), FR011 (Automatic Retry Logic)
        10. Test results export: Click "Export Results" button, verify CSV/JSON download works
        Expected: All UI components verified via screenshots, button clicks work, WebSocket connection stable, NO console errors, all Epic 1+2 functional requirements validated end-to-end
      </test-scenario>
      <test-scenario id="AC15" description="Document E2E Test Results and Production Readiness">
        Goal: Create comprehensive test report documenting job completion, metrics, issues, and production readiness assessment.
        Steps:
        1. Create test report file: docs/e2e-test-results-[date].md
        2. Document Job Completion Summary: Job ID, total URLs (10-20), success rate (expect &gt;95%), processing time (total and per URL, expect ~20 URLs/min with real APIs), cost breakdown (Gemini, GPT, total, expect &lt;$0.50 for test batch)
        3. Document Pre-Filter Rejection Rate: rejected_prefilter count / total_urls (expect 40-60%)
        4. Document LLM Usage: Gemini primary % (expect 70-90%), GPT fallback % (expect 10-30%)
        5. Attach screenshot evidence: Dashboard at 0%, 50%, 100% progress from Chrome DevTools MCP screenshots
        6. Document any errors or issues encountered: Failed URLs, API timeouts, rate limits, database connection issues, UI bugs
        7. Performance metrics: Processing rate (URLs/min), API latencies (ScrapingBee avg time, Gemini avg time, GPT avg time), Realtime latency (database write to UI update time)
        8. Production Readiness Assessment: Answer YES/NO to "Would I bet $100 this system is production-ready?" with reasoning. Address: System stability (any crashes?), Cost efficiency (within budget?), Error handling (graceful?), UI/UX quality (professional?), Real-time updates (working?)
        9. Recommendations for Production Deployment: Any config changes needed? API rate limit considerations? Monitoring requirements? Supabase Realtime stability observations?
        10. Mark Story 3.1 complete in story file if all acceptance criteria passed
        Expected: Comprehensive test report with metrics, screenshots, issues documented, production readiness assessment (YES/NO with evidence), recommendations for deployment
      </test-scenario>
    </ideas>
  </tests>
</story-context>
